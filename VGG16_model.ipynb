{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that our best model is made with the VGG16 pretrained model we can use a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 16\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "TRAIN_SPLIIT = 0.8\n",
    "CHANNELS= 3\n",
    "ES_EPOCHS = 30 \n",
    "CLASSES = 7\n",
    "EPOCHS= 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory with the images\n",
    "data_dir = \"C:\\\\Users\\\\Ronan\\\\Documents\\\\ML\\\\Taylor_Swift_Projects\\\\CNN\\\\Multiple_Class_Eras_Tour\"\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    labels='inferred',\n",
    "    seed = 42,\n",
    "    label_mode='int',\n",
    "    validation_split=None,\n",
    "    shuffle = True,\n",
    "    image_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size = BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the classes\n",
    "class_names = dataset.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_creation(dataset, train_split, test_split, val_split, shuffle=True, shuffle_size=100):\n",
    "\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "\n",
    "    size_dataset = len(dataset)\n",
    "\n",
    "    if shuffle:\n",
    "            dataset = dataset.shuffle(shuffle_size, seed=123)\n",
    "    \n",
    "    train_size = int(size_dataset * train_split)\n",
    "    test_size = int(size_dataset * test_split)\n",
    "    val_size = int(size_dataset * val_split)\n",
    "\n",
    "\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    test_dataset = dataset.skip(train_size).take(test_size)\n",
    "    val_dataset = dataset.skip(test_size).take(val_size)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = subset_creation(dataset, TRAIN_SPLIIT, TEST_SPLIT, VAL_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "    ----------------------------------\n",
    "    Dataset split:\n",
    "        - train split: {len(train_ds)}\n",
    "        - val split: {len(val_ds)}\n",
    "        - test split: {len(test_ds)}\n",
    "    ----------------------------------\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caching the dataset\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and rescale the images\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  tf.keras.layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "  tf.keras.layers.Rescaling(1./255),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proof that the images are rescaled\n",
    "normalized_ds = train_ds.map(lambda x, y: (resize_and_rescale(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import keras api \n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "  tf.keras.layers.RandomContrast(0.2),\n",
    "  #tf.keras.layers.RandomCrop(height=1, width=1),\n",
    "  tf.keras.layers.RandomZoom(0.2),\n",
    "  #tf.keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n",
    "  tf.keras.layers.RandomBrightness(factor=0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (data_augmentation(x, training=True), y)\n",
    ").prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "main_folder = 'Multiple_Class_Eras_Tour'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_counts = {}\n",
    "\n",
    "# Iterate through each subfolder (era)\n",
    "for era_folder in os.listdir(main_folder):\n",
    "    era_path = os.path.join(main_folder, era_folder)\n",
    "    if os.path.isdir(era_path):\n",
    "        # Count the number of images in the era folder\n",
    "        era_counts[era_folder] = len([filename for filename in os.listdir(era_path) if filename.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "\n",
    "# Create a pandas DataFrame from the era_counts dictionary\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'Era': era_counts.keys(), 'Image Count': era_counts.values()})\n",
    "\n",
    "# Create the bar plot using seaborn\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
    "ax = sns.barplot(x='Era', y='Image Count', data=df, palette='flare')  # Choose a color palette you like\n",
    "for bar in range(len(ax.containers)):\n",
    "    ax.bar_label(ax.containers[bar])\n",
    "# Customize the plot\n",
    "plt.title('Data Imbalance in Taylor Swift Eras Tour Images', fontsize=16)\n",
    "plt.xlabel('Eras', fontsize=14)\n",
    "plt.ylabel('Number of Images', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_inverse_num_samples(num_of_classes, samples_per_classes, coeff=1):\n",
    "    \"\"\"\n",
    "    Get the inverse number of samples for each class.\n",
    "    Args:\n",
    "        num_of_classes (int): Number of classes in the dataset.\n",
    "        samples_per_classes (list): List of number of samples per class.\n",
    "        power (int, optional): Power of the inverse number of samples. Defaults to 1.\n",
    "    Returns:\n",
    "        list: List of inverse number of samples for each class.\n",
    "    \"\"\"\n",
    "\n",
    "    inverse_num_samples = []\n",
    "    total_samples = sum(samples_per_classes)\n",
    "\n",
    "    for i in range(num_of_classes):\n",
    "        inverse_num_samples.append((total_samples / (num_of_classes * samples_per_classes[i])) * coeff)\n",
    "\n",
    "    return inverse_num_samples\n",
    "\n",
    "num_of_classes = 7\n",
    "samples_per_classes = [217, 230, 177, 153, 154, 194, 149] \n",
    "\n",
    "weighted_classes = get_weight_inverse_num_samples(num_of_classes, samples_per_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indexes = [1, 2, 3, 4, 5, 6, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = dict(zip(class_indexes, weighted_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used pretrained model VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = tf.keras.applications.vgg16.VGG16(\n",
    "weights=\"imagenet\",\n",
    "include_top=False,\n",
    "input_shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "for layer in conv_base.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "\n",
    "#tf.keras.backend.clear_session() # clear model number\n",
    "\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    resize_and_rescale,\n",
    "#    tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=input_shape),\n",
    "#    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "#    tf.keras.layers.Conv2D(filters=64,  kernel_size=(3,3), activation='relu'),\n",
    "#    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "#    tf.keras.layers.Conv2D(filters=128,  kernel_size=(3,3), activation='relu'),\n",
    "#    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "#    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "#    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "#    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "#    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "#    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "#    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "#    tf.keras.layers.Flatten(),\n",
    "#    tf.keras.layers.Dense(128, activation='relu'),\n",
    "#    tf.keras.layers.Dropout(0.5),\n",
    "#    tf.keras.layers.Dense(CLASSES, activation='softmax'),\n",
    "#])\n",
    "\n",
    "#model.build(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "x = resize_and_rescale(x)\n",
    "x = tf.keras.layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32, 64, 128, 256, 512]:\n",
    "\n",
    "    residual = x\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    x = tf.keras.layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    x = tf.keras.layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = tf.keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "    residual = tf.keras.layers.Conv2D(\n",
    "    size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = tf.keras.layers.add([x, residual])\n",
    "\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(CLASSES, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"saved_models/all_VGG16_models/best_model_weighted_VGG16.keras\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "#lr_plateau = callbacks.ReduceLROnPlateau(factor=0.8, monitor=\"val_accuracy\", patience=5)\n",
    "es = EarlyStopping(monitor='val_accuracy', patience=ES_EPOCHS)\n",
    "callbacks_list = [checkpoint, es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    "    class_weight=weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"saved_models/all_VGG16_models/best_model_weighted_VGG16.keras\"\n",
    "best_model = tf.keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = best_model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(EPOCHS), acc, label='Training Accuracy')\n",
    "plt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(EPOCHS), loss, label='Training Loss')\n",
    "plt.plot(range(EPOCHS), val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_y(dataset, model):\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "\n",
    "  for x, y in dataset:\n",
    "    y_true.append(y)\n",
    "    y_pred.append(tf.argmax(model.predict(x),axis = 1))\n",
    "    \n",
    "  y_pred = tf.concat(y_pred, axis=0)\n",
    "  y_true = tf.concat(y_true, axis=0)\n",
    "\n",
    "  return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "y_pred, y_true = create_list_y(test_ds, best_model)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "def read_file_as_img(file_path)-> np.ndarray:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    image = np.array(Image.open(BytesIO(data)))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\"Acoustic\", \"Fearless\", \"Folkmore\", \"Lover\", \"Midnights\", \"Reputation\", \"Speak Now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensorflow model\n",
    "PROD_MODEL_PATH = \"saved_models/all_VGG16_models/best_model_weighted_VGG16.keras\"\n",
    "\n",
    "PROD_MODEL = tf.keras.models.load_model(PROD_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = read_file_as_img(\"images/Fearless.jpg\")\n",
    "\n",
    "# Predict the image classification\n",
    "img_batch = np.expand_dims(img, 0)\n",
    "predictions = PROD_MODEL.predict(img_batch)\n",
    "\n",
    "predicted_class = predictions[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some of test dataset predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "    predictions = best_model.predict(img_array)\n",
    "\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
    "    return predicted_class, confidence\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in test_ds.take(1):\n",
    "    for i in range(6):\n",
    "        ax = plt.subplot(3, 2, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        \n",
    "        predicted_class, confidence = predict(model, images[i].numpy())\n",
    "        actual_class = class_names[labels[i]] \n",
    "        \n",
    "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
    "        \n",
    "        plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
